{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "import re\n",
    "import pymysql\n",
    "import numpy\n",
    "\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import SVD\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get top n recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving data from table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['avishal', 'J1Pa1E-G9HB7a-FEY0AjjA', '5.0'],\n",
       " ['avishal', '38Vtawjq7QjTMvJrKecQ9Q', '4.5'],\n",
       " ['avishal', '7a5-JFxoAEOZHuVqHMC3qQ', '4.5'],\n",
       " ['avishal', 'AkJVp9tEHwmIIB4uE7OndA', '5.0'],\n",
       " ['avishal', 'r5Db5G35arj7MkoleDyJCQ143', '5.0'],\n",
       " ['manish', 'iGqjf0SUKGrwVPrbMkNB-w130', '5.0'],\n",
       " ['manish', 'Bn-__FnS0rq0xXamhtuelQ', '3.5'],\n",
       " ['manish', 'U1WVMLpkFl2DcyKnnMorJQ', '2.0'],\n",
       " ['manish', 'Rlh9ZxBJhcOrnUZjTGWSvQ180', '5.0'],\n",
       " ['manish', '9uizR5S6-w4b5zhOugjL_A', '5.0'],\n",
       " ['manish', 'oLeRKgLaHE4km3c8QIfNrg7', '5.0'],\n",
       " ['hari', 'XZwRGkwGwG61S9o5oYZnSA', '5.0'],\n",
       " ['hari', 'xHnw7rFlQI2OcLUgL8IgGg', '1.5'],\n",
       " ['hari', 'OATENevK0cexjZ5qC1q2-A67', '4.5'],\n",
       " ['hari', 'C0OO1iYxHZiaGsVeh8dF9Q30', '4.5']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_file_path=\"C:/apache-tomcat-7.0.34/webapps/HW5_Arockiasamy_Vishal/\"\n",
    "cnx = pymysql.connect(user='root', password='12345',\n",
    "                              host='127.0.0.1',\n",
    "                              database='homehub')\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "query = (\"SELECT Login_ID, Product_ID, Review_Rating FROM transactions where Transaction_Status = 'Approved' and Order_Returned = 0\")\n",
    "cursor.execute(query)\n",
    "\n",
    "array_list = []\n",
    "for cus in cursor:\n",
    "    array_list.append(list(cus))\n",
    "array_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting data into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# field names  \n",
    "fields = ['userName', 'productName', 'reviewRating']\n",
    "    \n",
    "# name of csv file  \n",
    "filename = \"C:/apache-tomcat-7.0.34/webapps/HW5_Arockiasamy_Vishal/train_data.csv\"\n",
    "    \n",
    "# writing to csv file  \n",
    "with open(filename, 'w') as csvfile:   \n",
    "    csvwriter = csv.writer(csvfile)   \n",
    "    csvwriter.writerow(fields)  \n",
    "    csvwriter.writerows(array_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pr_file_path+\"/train_data.csv\", \"r\") as f:\n",
    "    reader = csv.DictReader(f, delimiter=',')\n",
    "    with open(pr_file_path+\"/test_data.csv\", \"w\",newline='') as f_out:\n",
    "        writer = csv.DictWriter(f_out, fieldnames=reader.fieldnames, delimiter=\",\")\n",
    "        for row in reader:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "file_path = os.path.expanduser(pr_file_path+'/test_data.csv')\n",
    "\n",
    "# As we're loading a custom dataset, we need to define a reader. In the\n",
    "# movielens-100k dataset, each line has the following format:\n",
    "# 'user item rating timestamp', separated by '\\t' characters.\n",
    "reader = Reader(line_format='user item rating', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving top five recommendation for the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avishal ['Rlh9ZxBJhcOrnUZjTGWSvQ180', 'iGqjf0SUKGrwVPrbMkNB-w130', 'oLeRKgLaHE4km3c8QIfNrg7', 'XZwRGkwGwG61S9o5oYZnSA', '9uizR5S6-w4b5zhOugjL_A']\n",
      "manish ['7a5-JFxoAEOZHuVqHMC3qQ', 'C0OO1iYxHZiaGsVeh8dF9Q30', 'XZwRGkwGwG61S9o5oYZnSA', 'r5Db5G35arj7MkoleDyJCQ143', 'AkJVp9tEHwmIIB4uE7OndA']\n",
      "hari ['iGqjf0SUKGrwVPrbMkNB-w130', '9uizR5S6-w4b5zhOugjL_A', 'r5Db5G35arj7MkoleDyJCQ143', 'oLeRKgLaHE4km3c8QIfNrg7', 'Rlh9ZxBJhcOrnUZjTGWSvQ180']\n"
     ]
    }
   ],
   "source": [
    "# First train an SVD algorithm on the movielens dataset.\n",
    "data = Dataset.load_from_file(file_path, reader=reader)\n",
    "trainset = data.build_full_trainset()\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Than predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "testset = trainset.build_anti_testset()\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "top_n = get_top_n(predictions, n=5)\n",
    "\n",
    "# Print the recommended items for each user\n",
    "for uid, user_ratings in top_n.items():\n",
    "    print(uid, [iid for (iid, _) in user_ratings])\n",
    "    \n",
    "out = open(pr_file_path+'/MatrixFactorization.csv', 'w',newline='')\n",
    "output=csv.writer(out)\n",
    "\n",
    "for uid, user_ratings in top_n.items():\n",
    "    output.writerow([uid, [iid for (iid, _) in user_ratings]])\n",
    "    \n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/apache-tomcat-7.0.34/webapps/HW5_Arockiasamy_Vishal/test_data.csv'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
